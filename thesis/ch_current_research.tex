\chapter{相关研究现状分析}


\section{数据立方的计算研究}

数据立方的计算涉及到对原事实表数据进行扫描，对相应的 GROUP BY 属性值执行聚合计算以及生成相应的立方数据等几个步骤。在立方计算的问题讨论中，如何尽可能地减少原事实表的数据扫描次数、降低 I/O 代价以及计算过程中内存的开销，成为评价一个立方计算方法高效性的几个关键要点。

GBLP \cite{gray1997data}、PipeSort、PipeHash \cite{agarwal1996computation}、BUC \cite{beyer1999bottom} 以及 Partitioned-Cube \cite{ross1997fast} 是几种经典的数据立方计算方法。 其中GBLP、 PipeSort、 PipeHash以及Partitioned-Cube 均是自顶向下的计算，而 BUC 则是自底向下的计算方法。如图 \ref{4_dimension_lattice}，展示了一个 4 维数据立方的 lattice，其中 A，B，C，D 均为属性维度。在一个数据立方 lattice 中，每一个节点代表一个数据小方，箭头从父节点指向子节点。自顶向下，顾名思义就是在一个 lattice 中从父节点开始往下计算，而自底向上则相反。

\begin{figure}[!htb]
\centering\includegraphics[width=3in]{picture/ch_current_research/4_dimension_lattice} 
\caption{4维数据立方的Lattice}\label{4_dimension_lattice} 
\end{figure} 

\begin{itemize}

\item \textbf{GBLP}

Jim Gary 等人在提出数据立方概念的同时，还提出了一种计算数据立方的算法 —— GBLP[12]。该算法的核心思想是自顶向下，为每个小方选择其最小的父小方(指小方内group数量最少)作为其计算的基础。对于一个立方 lattice，越往下，小方内group的数量就会变得越少，因此相对于每个小方都基于原始数据表进行计算，基于最小父小方的计算，因为数据量的大规模减少而加快了一个小方的计算速度。但该方法只是把一个 lattice 修剪成一个树型结构，却没有提供一个特定的 lattice 树遍历计划。另外当计算数据量很大无法完全存储于内存时，该方法没有提供一个有效的磁盘访问策略而使得 I/O 代价会很高。

\item \textbf{PipeHash}

1996 年，Agrarwal 等人针对 GBLP 算法的缺陷，提出了第一个改进的数据立方计算方法 —— PipeSort\cite{agarwal1996computation} 。在一个数据立方 lattice 中，假如一个子小方的维属性排序顺序是它其中一个父小方的维属性排序顺序的前缀，则该子小方可以在其父小方的基础上计算而无需额外的排序。如前面提到的图 2 所示的数据立方，假设每个小方的维属性排序顺序就是图示的小方名字从左到右的维顺序，例如对于小方 ABC，首先按照维属性 A 排序，若 A 相同的情况下再按照 B 排序，以此类推。小方 AB 的维顺序是小方 ABC 的前缀，但小方 AC 不是小方 ABC 的前缀。在这样的情况下，AB 可从 ABC 计算获得，ABC 可从 ABCD 计算获得，于是形成了流水线 ABCD-\textgreater ABC-\textgreater AB-\textgreater A。因此对原数据和一些小方进行多次排序，即可生成多条流水线。

PipeSort 在 GBLP 的基础上，通过采样估算每个小方的大小，通过二分匹配的方法找出每个子小方需要从哪个父小方计算获得。然后基于这个二分匹配的结果将一个 lattice 切割成多棵子树，称为 pipeline (流水线），pipeline 内只有根节点需要对输入数据进行排序。对于每个 pipeline，从根节点逐层往下计算，除根节点外，只需在内存中为每个节点缓存一条元组，极大的提高了内存的使用率。只是，当数据立方的维度变得很高时，PipeSort 的排序代价则会成指数增长。 且当数据比较稀疏时，排序的中间结果不能完全存储于内存中，还需进行外部排序，增加了 I/O 代价。图\ref{pipesort} 为 4 维数据立方使用 pipesort 的计算过程。图(a)中的虚线表示从父小方排序后得到子小方的过程，实线表示流水线的计算过程。图(b)是最终需要计算的流水线，椭圆中的小方表示需要重新排序。

以上提到的几种算法都没有考虑到数据稀疏的情况，在现实应用中，事实表往往是稀疏的。针对基于稀疏基表的数据立方的有效计算 ，Ross 等人在1997 年提出了 Partitioned-Cube 算法[20]。该算法采用分治的策略，对不能完
全存储与内存的数据集根据某个候选聚集维属性进行划分，划分成多个子集，
因为数据是稀疏的，所以能使得每个子集均能在内存中计算，于是通过此次划
分能够计算所有和该维属性相关的小方结果。完成计算后，将该划分维从候选
聚集维中去除，根据上述方法重复计算基于剩余维属性的小方结果直至立方计
算完毕。相对于前几种算法，这种算法在数据稀疏的情况下具有更小的 I/O 代
价，且受维度增加的性能影响较少。

\begin{figure}[!htb]
\centering\includegraphics[width=5in]{picture/ch_current_research/pipesort} 
\caption{4维数据立方 pipesort 计算过程}\label{pipesort} 
\end{figure} 

\item \textbf{PipeHash}

Agrarwal 等人还提出了 PipeHash[19]算法，该算法是 PipeSot 算法的一个变形，基于 Hash(散列)实现的。PipeHash 算法首先会根据 GBLP 算法的思想将一个立方 lattice 修剪成一棵 lattice 执行树。为了使得在同一个聚集计算里的所有元组能够在内存上相邻的位置，PipeHash 会为每一个同时计算的小方维护一个 Hash 表。假如有些小方的散列表不能同时存放在内存里，则 PipeHash 算法就会把一个 lattice 执行树划分成多棵子树，使得每棵子树内所有小方的 Hash表都能够同时存储于内存中。 由于划分代价要比排序代价要小，所以PipeHash 的性能要比 PipeSort 要高。但 PipeSort 用一个排序可以计算多个聚集结果，而 PipeHash 则每次都需要对数据进行重新 Hash，且 PipeHash 相对于PipeSort 需要大量的内存来存储每棵子树内所有小方的 Hash 表。

\item \textbf{Partitioned-Cube}

以上提到的几种算法都没有考虑到数据稀疏的情况，在现实应用中，事实表往往是稀疏的。针对基于稀疏基表的数据立方的有效计算 ，Ross 等人在1997 年提出了 Partitioned-Cube 算法 \cite{ross1997fast}。该算法采用分治的策略，对不能完全存储于内存的数据集根据某个候选聚集维属性进行划分，划分成多个子集，因为数据是稀疏的，所以每个子集均能在内存中计算，于是通过此次划分能够计算所有和该维属性相关的小方结果。完成计算后，将该划分维从候选聚集维中去除，根据上述方法重复计算基于剩余维属性的小方结果直至立方计算完毕。相对于前几种算法，这种算法在数据稀疏的情况下具有更小的 I/O 代价，且受维度增加的性能影响较少。图\ref{partitioned_cube} 为 4 维数据使用 Partitioned Cube计算的过程。

\begin{figure}[!htb]
\centering\includegraphics[width=3.5in]{picture/ch_current_research/partitioned_cube} 
\caption{Partitioned Cube 的计算过程}\label{partitioned_cube} 
\end{figure} 

\item \textbf{BUC}

相对于上述几种自顶向下的立方计算算法，Beyer 等人在 1999 年提出的 BUC （Bottom Up Cube) 则是一种自底向上的算法。BUC 算法是在 Partitioned-Cube 算法的思想上，针对冰山数据立方，即基于某个最少阈值的 GROUP BY 计算而提出的，也即具有 HAVING 条件的 GROUP BY。例如 HAVING COUNT(*) > 100 这样的条件限制。大部分的度量函数是具有单调性的，例如 COUNT()，若 GROUPBY(A=a0) 的结果是小于 100，那么 GROUPBY(A=a0， B=b0) 的结果必然是小于 100。正是这种单调性，加上有阈值限制，将数据立方的计算变成从底向上，可以实现一定的剪枝。例如上述的例子 COUNT(*) > 100，如果 GROUPBY(A=a0)是小于 100 的，那么与 A=a0 相关的 group 就不需要计算了。

BUC算法会根据聚集维属性对基表数据进行多次分组并计算聚集结果，并会基于维顺序对分组进行进一步的细分计算。因为在计算过程中考虑了阈值的问题，所以从 lattice 的底部越往上，小方需要进行聚集计算的数据就会变得越少。由于 PipeSort，PipeHash，PartitionedCube 在计算稀疏数据立方时都会产生相对较大的 I/O 和内存消耗代价，所以 BUC 算法在计算稀疏矩阵时的性能要比以上几种算法性能要好。图 \ref{BUC_partition} 是 BUC 根据属性 A 进行的划分。图 \ref{BUC_execution} 是 BUC 的执行过程。在图 \ref{BUC_execution} 中，原数据分别按照 A， B， C， D 这4个属性进行划分。在属性 A 的划分中，计算所有与属性 A 相关的数据小方，当出现一个小方不满足阈值时，则往上的小方都不需要计算了。

\begin{figure}[!htb]
\centering\includegraphics[width=1.7in]{picture/ch_current_research/BUC_partition} 
\caption{BUC 使用维属性的划分}\label{BUC_partition} 
\end{figure} 


\begin{figure}[!htb]
\centering\includegraphics[width=3in]{picture/ch_current_research/BUC_execution} 
\caption{BUC 的执行过程}\label{BUC_execution} 
\end{figure} 

\end{itemize}

\section{部分数据立方的计算研究}

在前文提到，当一个数据立方的维度为 D 时，数据立方内就包含了 ${2}^{D}$ 个数据小方，也即 ${2}^{D}$ GROUP BY 计算。当维度增加时，数据立方的计算和存储代价就会呈指数增长。在现实条件下，计算时间和存储空间往往都受限制，而且对于某些数据小方，在现实中也常常不是 OLAP 分析人员需要的数据。因此可选择只计算部分的数据立方，在这里提到的部分数据立方，是指在立方实现与存储时选择了部分小方而非所有小方的数据立方。当查询的数据小方已经计算了，则可直接查询获得；若查询的数据小方未计算，则从已计算的数据小方中计算获得，而非原数据。于是，如何合理的选择部分立方进行计算就成了部分数据立方计算实现和查询相应速度的关键。

Harinarayan 等人在1996 年提出的贪心算法 BPUS \cite{harinarayan1996implementing} ，通过单位空间效益的指标来对小方节点进行贪心选择。该算法比穷尽搜索要快，但算法复杂度较高，随立方维度增加算法时间复杂度呈指数增长。 Shukla等人在 1998 年基于 BPUS 算法思想提出了一个使用简单启发式搜索的改进算法 PBS \cite{shukla1998materialized} ，该算法时间复杂度较低其运行速度比 BPUS 要快几个数量级，且它能够容易的与自底向上的立方计算方法整合，但该算法会为了加快其运行速度而牺牲了一定的结果质量。除了以上几个贪心算法外，业界也提出了其他的优化贪心算法来进行小方选择。同时，除了从存储代价角度来考虑立方选择之外，业界也尝试从立方维护代价等多个角度进行小方选择算法的设计与优化。

\section{分布式数据立方的计算研究}

随着计算量的极速增长，对于 OLAP 技术中的关键技术 —— 数据立方，业界也开始研究利用分布式计算的优势实现数据立方的方法。\cite{nandi2011distributed} \cite{dehne2006cgmcube} \cite{ng2011iceberg} \cite{lee2012efficient}。

cgmCUBE \cite{dehne2006cgmcube}、RP、BPP、ASL \cite{lee2012efficient} 这几个算法是基于由少量 PC 机组成的集群实现的，无法适应 TB 级的处理数据量，也并没有与MapReduce框架相结合，同时无法处理整体性度量计算。

MR-CUBE[5][14]是基于 MapReduce 提出的数据立方算法，该算法结合了MapReduce 与数据立方计算的特点，能够达到高效实现 TB 级数据量的数据立方计算。MR-CUBE 针对整体性度量 measure 提出了相应的分布式计算思路，并指出了其中 2 种整体性度量 measure 实际的分布式计算处理方法。 为了避免MapReduce 集群节点间的数据分布不均衡，MR-CUBE 采用对基表进行采样的方法来决定如何在后续计算中对数据小方进行切割以使得集群节点间的装载平衡。利用数据立方小方间计算过程可共享的特性， MR-CUBE 对原数据立方lattice 进行分支，使得每个分支能在共享一套数据的基础上进行 BUC 计算，从而降低了 MapReduce 的中间处理数据量。

虽然 MR-CUBE 是目前基于 MapReduce 框架一个较好的数据立方实现尝试。但与此同时，MR-CUBE 现有的一些不足不得不提。MR-CUBE 的数据切割是基于整个数据小方的，当数据小方内某个元组的计算数据过度倾斜，而其他元组的计算数据都较小时，对整个数据小方进行切割就会导致其他小方元组不必要的切割。过多的切割会导致后续的合并计算过多，当数据过度倾斜时，这种切割方式的劣势就会显得十分明显。此外，MR-CUBE 的 lattice 分支方式是基于手工经验值，当数据立方维度过高时，自动有效的划分合理的分支就显得十分必要。而且，在 MR-CUBE 的实验中只探讨了 2 种整体性度量方法的分布式计算方法，提出更多个整体性度量方法的分布式计算方式，甚至是一般化的、自动化的整体性度量分布式计算有助于日后的 MapReduce Cube 一般化实现。另外，MR-CUBE 的实验也只是探讨了 2 到 3 维的低维数据立方的实现，对于高维的数据立方实现，MR-CUBE 算法可能会暴露更多的不足。

除了 MR-CUBE 外，此外针对 MapReduce 框架进行数据立方分布式实现的算法研究还有 MRPipeLevel[18] ，Parallel Closed Cubeing[24] 。 其 中MRPipeLevel 是基于 PipeSort 实现的，而 Parallel Closed Cubeing 则是针对封闭立方体(Closed Cube)[40]，一种基于无损压缩的数据立方体实现方法。
